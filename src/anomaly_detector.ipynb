{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "all_columns = [\"mapped_veh_id\",\n",
    "               \"timestamps_UTC\",\n",
    "               \"lat\",\"lon\",\n",
    "               \n",
    "               \"RS_E_OilPress_PC1\",\"RS_E_OilPress_PC2\",\n",
    "\n",
    "               \"RS_E_RPM_PC1\",\"RS_E_RPM_PC2\",\n",
    "\n",
    "               \"RS_E_InAirTemp_PC1\",\"RS_E_InAirTemp_PC2\",\n",
    "               \"RS_E_WatTemp_PC1\",\"RS_E_WatTemp_PC2\",\n",
    "               \"RS_T_OilTemp_PC1\",\"RS_T_OilTemp_PC2\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['mapped_veh_id', 'timestamps_UTC', 'lat', 'lon', 'RS_E_InAirTemp_PC1',\n",
       "       'RS_E_InAirTemp_PC2', 'RS_E_OilPress_PC1', 'RS_E_OilPress_PC2',\n",
       "       'RS_E_RPM_PC1', 'RS_E_RPM_PC2', 'RS_E_WatTemp_PC1', 'RS_E_WatTemp_PC2',\n",
       "       'RS_T_OilTemp_PC1', 'RS_T_OilTemp_PC2', 'z_score'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_types = {\"mapped_veh_id\": np.int32,\n",
    "            #\"RS_E_OilPress_PC1\": np.int32,\n",
    "            #\"RS_E_OilPress_PC2\": np.int32,\n",
    "            #\"RS_E_RPM_PC1\": np.int32,\n",
    "            #\"RS_E_RPM_PC2\": np.int32,\n",
    "            #\"RS_E_InAirTemp_PC1\": np.int32,\n",
    "            #\"RS_E_InAirTemp_PC2\": np.int32,\n",
    "            #\"RS_E_WatTemp_PC1\": np.int32,\n",
    "            #\"RS_E_WatTemp_PC2\": np.int32,\n",
    "            #\"RS_T_OilTemp_PC1\": np.int32,\n",
    "            #\"RS_T_OilTemp_PC2\": np.int32\n",
    "            }\n",
    "data = pd.read_csv(\"../data/cleaned_sorted_full_data.csv\", delimiter=\";\", index_col=False, dtype=col_types)\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#____________________CLEANING THE CSV FILE\n",
    "\n",
    "def clean_csv(data, output_file):\n",
    "\n",
    "    # Filter out rows where 'RS_E_InAirTemp_PC1' is greater than or equal to 30000\n",
    "    cleaned_data = data[data['RS_E_InAirTemp_PC1'] < 30000]\n",
    "\n",
    "    # Filter out rows where 'RS_E_InAirTemp_PC2' is greater than or equal to 1000\n",
    "    cleaned_data = data[data['RS_E_InAirTemp_PC2'] < 1000]\n",
    "\n",
    "\n",
    "    # Filter out rows where 'RS_E_OilPress_PC1' is greater than or equal to 1000\n",
    "    cleaned_data = data[data['RS_E_OilPress_PC1'] < 689]\n",
    "\n",
    "    \n",
    "\n",
    "    # Write the cleaned data to a new CSV file\n",
    "    cleaned_data.to_csv(output_file, index=False, sep=';')\n",
    "\n",
    "output_file_path = \"../data/cleaned_sorted_full_data_bis.csv\"\n",
    "clean_csv(data,output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_______________UAD (Univariate Anomaly Detection):\n",
    "\"\"\"\n",
    "    This method looks at one feature at a time to identify anomalies. If the anomalies can be detected by examining \n",
    "    individual metrics, then we can apply statistical methods to do this. Methods like:\n",
    "        -Z-Score: Measures how many standard deviations a data point is from the mean. Data points with an absolute z-score above a threshold (e.g., 3) \n",
    "            are considered outliers.\n",
    "        -IQR (Interquartile Range): It's the range between the first quartile and the third quartile. Data points below (1st quartile - 1.5 x IQR) or \n",
    "            above (3rd quartile + 1.5 x IQR) can be considered outliers.\n",
    "\"\"\"\n",
    "\n",
    "def z_score_anomaly_detection(chunk, column_name, threshold=2):\n",
    "    \"\"\"\n",
    "    Detects anomalies in a given chunk based on z-score.\n",
    "    The basic idea is to determine mean/standard deviation value to calculate the z-score.\n",
    "    The z-score tells us how far the measured value is far from the mean value (how many standard deviations)\n",
    "    If the corresponding z-score is higher than 2-3 time the standard deviation we count the measure as an anomaly.\n",
    "    This technique is usefull when we can notice errors in one column.\n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    mean = chunk[column_name].mean()\n",
    "    std_dev = chunk[column_name].std()\n",
    "\n",
    "    \n",
    "    # Calculate the z-scores\n",
    "    chunk['z_score'] = (chunk[column_name] - mean) / std_dev\n",
    "\n",
    "    # Filter rows where absolute z-score is greater than the threshold\n",
    "    anomalies = chunk[chunk['z_score'].abs() > threshold]\n",
    "    \n",
    "    return anomalies, mean, std_dev\n",
    "\n",
    "def write_correct_informations(output_file,name,mean,std_dev,len_data,len_anomalies,ratio_of_issues,treshold_min,treshold_max,veh_id,probabilty_to_appears):\n",
    "    \n",
    "    with open(f'../Adel_temp/{output_file}', 'a') as file:\n",
    "        file.write(f\"column_name ({name}) with a threshold ({2}): \\n\\t-The mean value: {mean} \\n\\t-The standard deviation: {std_dev}\\n\")\n",
    "        file.write(f\"Here's the setup: \\n\\t-data: {len_data} \\n\\t-anomalies: {len_anomalies} \\n\\t-ratio: {(ratio_of_issues):.2f}% \\n\\t-treshold min: {treshold_min} \\n\\t-treshold max: {treshold_max}\\n\")\n",
    "\n",
    "        file.write(f\"\\t\\t-Vehicle ID {veh_id} appeared {(probabilty_to_appears):.2f}% of times in the anomalie {name}.\\n\")\n",
    "        file.write('______'*10 + \"\\n\\n\")\n",
    "\n",
    "def clean_files(file_name_list):\n",
    "    for file in file_name_list:\n",
    "        with open(f'../Adel_temp/{file}', 'w') as file:\n",
    "            file.write(\"\") #Just to clean the file\n",
    "\n",
    "def interprete_data_and_write(data,treshold_min,treshold_max):\n",
    "    # Open the file for writing\n",
    "    \n",
    "    current_all_columns = [\n",
    "        \"mapped_veh_id\",\n",
    "        \"lat\", \"lon\",\n",
    "        \"RS_E_OilPress_PC1\", \"RS_E_OilPress_PC2\",\n",
    "        \"RS_E_RPM_PC1\", \"RS_E_RPM_PC2\",\n",
    "        \"RS_E_InAirTemp_PC1\", \"RS_E_InAirTemp_PC2\",\n",
    "        \"RS_E_WatTemp_PC1\", \"RS_E_WatTemp_PC2\",\n",
    "        \"RS_T_OilTemp_PC1\", \"RS_T_OilTemp_PC2\"\n",
    "    ]\n",
    "\n",
    "    file_name_list = ['0_under_treshold.txt','1_between_treshold.txt','2_over_treshold.txt']\n",
    "    clean_files(file_name_list)\n",
    "\n",
    "    #treshold of trains that got issues knowing we get issues\n",
    "        #treshold_min = 0.05\n",
    "        #treshold_max = 0.1\n",
    "\n",
    "    for name in current_all_columns:\n",
    "        anomalies, mean, std_dev = z_score_anomaly_detection(data, name)\n",
    "        ratio_of_issues = len(anomalies)/len(data)\n",
    "\n",
    "        # Count occurrences of each unique mapped_veh_id\n",
    "        veh_id_counts = anomalies['mapped_veh_id'].value_counts()\n",
    "        \n",
    "\n",
    "        # Write the counts of each mapped_veh_id to the file\n",
    "        for veh_id, count in veh_id_counts.items():\n",
    "            probabilty_to_appears = count/(len(anomalies))\n",
    "\n",
    "            if probabilty_to_appears< treshold_min:\n",
    "                output_file = file_name_list[0]\n",
    "            elif treshold_min <= probabilty_to_appears < treshold_max:\n",
    "                output_file = file_name_list[1]\n",
    "            else:\n",
    "                output_file = file_name_list[2]\n",
    "\n",
    "            write_correct_informations(output_file,name,mean,std_dev,len(data),len(anomalies),ratio_of_issues,treshold_min,treshold_max,veh_id,probabilty_to_appears)\n",
    "\n",
    "treshold_min = 0.05\n",
    "treshold_max = 0.1\n",
    "interprete_data_and_write(data,treshold_min,treshold_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install plotly\n",
    "#!pip install nbformat>=4.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nIn the context of trains/metros, we might want to consider methods designed for such data.\\n    -Exponential Smoothing, ARIMA: Model the time-series and then check for data points that deviate significantly from the predicted values.\\n    -Moving Average: Calculate the moving average and detect points that deviate from this average by more than a predefined threshold.\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "#_______________MAD (Multivariate Anomaly Detection):\n",
    "\"\"\"\n",
    "This looks at combinations of features to detect anomalies.\n",
    "\n",
    "    -PCA (Principal Component Analysis): Transform data into principal components and then analyze residuals (difference between original and reconstructed data) to detect \n",
    "        anomalies.\n",
    "    -Clustering (e.g., K-means): Data points that are far from cluster centers can be considered anomalies.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#_______________TSAD (Time-Series Anomaly Detection):\n",
    "\"\"\"\n",
    "In the context of trains/metros, we might want to consider methods designed for such data.\n",
    "    -Exponential Smoothing, ARIMA: Model the time-series and then check for data points that deviate significantly from the predicted values.\n",
    "    -Moving Average: Calculate the moving average and detect points that deviate from this average by more than a predefined threshold.\n",
    "\"\"\"\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
